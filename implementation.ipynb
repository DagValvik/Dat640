{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline using BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sigurd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "PUNCTUATIONS = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101093\n",
      "808731\n"
     ]
    }
   ],
   "source": [
    "# Queries dev, train or eval\n",
    "# queriesDev = \"data/queries/queries.dev.tsv\"\n",
    "queriesDev = \"D:/DAT640/queries.dev.tsv\"\n",
    "queries_dev_df = pd.read_csv(queriesDev, sep='\\t', header=None)\n",
    "print(len(queries_dev_df))\n",
    "\n",
    "# NOTE Not used\n",
    "# queriesTrain = \"data/queries/queries.train.tsv\"\n",
    "# queriesTrain = \"D:/DAT640/queries.train.tsv\"\n",
    "# queries_train_df = pd.read_csv(queriesTrain, sep='\\t', header=None)\n",
    "# print(len(queries_train_df))\n",
    "\n",
    "# queriesEval = \"data/queries/queries.eval.tsv\"\n",
    "# queriesEval = \"D:/DAT640/queries.eval.tsv\"\n",
    "# queries_eval_df = pd.read_csv(queriesEval, sep='\\t', header=None)\n",
    "# print(len(queries_eval_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passages to rank based on query\n",
    "# collectionFile = \"data/collection/collection.tsv\"\n",
    "collectionFile = \"D:/DAT640/collection.tsv\"\n",
    "collection_df = pd.read_csv(collectionFile, sep='\\t', header=None) #, index_col=0)\n",
    "# len(collection_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The presence of communication amid scientific ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Manhattan Project and its atomic bomb help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Essay on The Manhattan Project - The Manhattan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Manhattan Project was the name for a proje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>versions of each volume as well as complementa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1\n",
       "0  0  The presence of communication amid scientific ...\n",
       "1  1  The Manhattan Project and its atomic bomb help...\n",
       "2  2  Essay on The Manhattan Project - The Manhattan...\n",
       "3  3  The Manhattan Project was the name for a proje...\n",
       "4  4  versions of each volume as well as complementa..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qrelsDev = \"data/qrels.dev.tsv\"\n",
    "qrelsDev = \"D:/DAT640/qrels.dev.tsv\"\n",
    "qrels_dev_df = pd.read_csv(qrelsDev, sep='\\t', header=None)\n",
    "qrels_dev_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "ql = []\n",
    "pl = []\n",
    "for r in qrels_dev_df.iterrows():\n",
    "    qid = r[1][0]\n",
    "    pid = r[1][2]\n",
    "    ql.append(queries_dev_df.loc[queries_dev_df[0]==qid])\n",
    "    pl.append(collection_df.loc[collection_df[0]==pid])\n",
    "\n",
    "qDF = pd.concat(ql,ignore_index=True)\n",
    "colq =[\"qid\", \"query\"]\n",
    "qDF.columns = colq\n",
    "\n",
    "pDF = pd.concat(pl,ignore_index=True)\n",
    "colp =[\"pid\", \"passages\"]\n",
    "pDF.columns = colp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sample = qDF\n",
    "passage_sample = pDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. what is a corporation?'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_id = np.array(query_sample.iloc[:, 0])\n",
    "queries = np.array(query_sample.iloc[:, -1])\n",
    "queries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"McDonald's Corporation is one of the most recognizable corporations in the world. A corporation is a company or group of people authorized to act as a single entity (legally a person) and recognized as such in law. Early incorporated entities were established by charter (i.e. by an ad hoc act granted by a monarch or passed by a parliament or legislature).\""
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passages_id = np.array(passage_sample.iloc[:, 0])\n",
    "passages = np.array(passage_sample.iloc[:, -1])\n",
    "passages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenized_corpus = []\n",
    "\n",
    "    for doc in corpus:\n",
    "        # Remove specific punctuations\n",
    "        for punctuation in PUNCTUATIONS:\n",
    "            doc = doc.replace(punctuation, \" \")\n",
    "\n",
    "        # Get only the words, not the whitespace\n",
    "        words = [word for word in doc.split(\" \") if word]\n",
    "\n",
    "        # Remove specific stopwords\n",
    "        words = [word for word in words if word not in STOPWORDS]\n",
    "\n",
    "        # Add to the list of tokenized docs\n",
    "        tokenized_corpus.append(words)\n",
    "        \n",
    "\n",
    "    return tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['corporation'],\n",
       " ['rachel', 'carson', 'write', 'obligation', 'endure'],\n",
       " ['rachel', 'carson', 'write', 'obligation', 'endure']]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_queries = tokenize(queries)\n",
    "tokenized_queries[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colTokenizedDF = pd.DataFrame(columns=[\"pid\", \"passage\"])\n",
    "# count = 0\n",
    "# with open(collectionFile, encoding=\"utf-8\") as f:\n",
    "#     # f.readline(4)\n",
    "#     for line in f:\n",
    "#         if count % 100000 == 0:\n",
    "#             print(count)\n",
    "#         count += 1\n",
    "#         # print(line.split(\"\\t\"))\n",
    "#         tokenized = tokenize(line[1])\n",
    "#         colTokenizedDF = pd.concat([colTokenizedDF, pd.DataFrame.from_dict({\"pid\": line[0], \"passage\": tokenized})], ignore_index=True)\n",
    "#         # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_passages = tokenize(passages)\n",
    "# tokenized_passages[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to look up id\n",
    "query_lookup = {}\n",
    "for idx, query in enumerate(queries):\n",
    "    query_lookup[query] = queries_id[idx]\n",
    "\n",
    "# tokenized_query_lookup = {}\n",
    "# for idx, (tokenized, query) in enumerate(zip(tokenized_queries, queries)):\n",
    "#     tokenized_query_lookup[\" \".join(tokenized)] = queries_id[idx]\n",
    "\n",
    "passage_lookup = {}\n",
    "for idx, passage in enumerate(passages):\n",
    "    passage_lookup[passage] = passages_id[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 Implementation\n",
    "- https://pypi.org/project/rank-bm25/\n",
    "- http://www.cs.otago.ac.nz/homepages/andrew/papers/2014-2.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(tokenize(passages))\n",
    "bm25_rankings = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Get ground truth\n",
    "def load_ground_truth(qrels_df : pd.DataFrame) -> dict[str, set[str]]:\n",
    "    qrels_dict = {}\n",
    "    for idx, row in qrels_df.iterrows():\n",
    "        qid = row[0]\n",
    "        pid = row[2]\n",
    "        relevance = row[3]\n",
    "        if relevance > 0:\n",
    "            if qid not in qrels_dict:\n",
    "                qrels_dict[qid] = set()\n",
    "            qrels_dict[qid].add(pid)\n",
    "    return qrels_dict\n",
    "\n",
    "ground_truth = load_ground_truth(qrels_dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_1000(query_id, bm_25_rankings, ground_truth):\n",
    "    bm_25 = set(bm_25_rankings[query_id])\n",
    "    truth = ground_truth[query_id]\n",
    "    prec = len(bm_25.intersection(truth)) / len(bm_25)\n",
    "    return prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_precision_1000(query_id, bm_25_rankings, ground_truth) -> float:\n",
    "    vals = []\n",
    "    over = 1\n",
    "    for rank_idx, rank in enumerate(bm_25_rankings[query_id]):\n",
    "        under = rank_idx+1\n",
    "        if rank in ground_truth[query_id]:\n",
    "            vals.append(over / under)\n",
    "            over += 1\n",
    "    AP = sum(vals) / len(ground_truth[query_id])\n",
    "\n",
    "    return AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reciprocal_rank(query_id, bm_25_rankings, ground_truth) -> float:\n",
    "    AP = 0\n",
    "    for rank_idx, rank in enumerate(bm_25_rankings[query_id]):\n",
    "        under = rank_idx+1\n",
    "        if rank in ground_truth[query_id]:\n",
    "            AP = 1 / under\n",
    "            break\n",
    "\n",
    "    return AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_eval_measure(bm_25_rankings, ground_truths, eval_function) -> float:\n",
    "    results = []\n",
    "    for query in bm_25_rankings:\n",
    "        results.append(eval_function(bm_25_rankings[query], ground_truths[query]))\n",
    "    return sum(results) / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corporation']"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_queries = 6\n",
    "query_tokens = tokenized_queries[0:num_queries]\n",
    "query_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, query in enumerate(query_tokens):\n",
    "    top_1000 = bm25.get_top_n(query, passages, n=5)\n",
    "    query_index = query_lookup[queries[idx]]\n",
    "    bm25_rankings[query_index] = [passage_lookup[passage_key] for passage_key in top_1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = pd.DataFrame(columns=[\"query\", \"precision\", \"avg_precision\", \"reciprocal_rank\"])\n",
    "m =[]\n",
    "for query_id in bm25_rankings:\n",
    "    score = get_precision_1000(query_id, bm25_rankings, ground_truth)\n",
    "    pres = get_average_precision_1000(query_id, bm25_rankings, ground_truth)\n",
    "    rec = get_reciprocal_rank(query_id, bm25_rankings, ground_truth)\n",
    "    m.append((query_id, score, pres, rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID\tRankings\t\t\t\t\tGround truth\n",
      "1102432\t\t5501299 5501299 6456242 6419354 6419354\t\t2026790\n",
      "1102431\t\t1459230 7452193 7975471 7970517 7680370\t\t7066866 7066867\n",
      "1090282\t\t7066900 426316 7622282 7185364 7622280\t\t7066900\n",
      "39449\t\t7066905 7773569 7501532 7999377 7904147\t\t7066905\n",
      "76162\t\t7467195 7066915 7755622 7443562 7755288\t\t7066915\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query ID\\tRankings\\t\\t\\t\\t\\tGround truth\")\n",
    "for qid in bm25_rankings:\n",
    "    r = \" \".join([str(v) for v in bm25_rankings[qid]])\n",
    "    g = \" \".join([str(v) for v in ground_truth[qid]])\n",
    "    print(f\"{qid}\\t\\t{r}\\t\\t{g}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID\tpre\tavg_pre\trr\n",
      "1102432\t\t0.0\t0.0\t0\n",
      "1102431\t\t0.0\t0.0\t0\n",
      "1090282\t\t0.2\t1.0\t1.0\n",
      "39449\t\t0.2\t1.0\t1.0\n",
      "76162\t\t0.2\t0.5\t0.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query ID\\tpre\\tavg_pre\\trr\")\n",
    "for vals in m:\n",
    "    print(f\"{vals[0]}\\t\\t{round(vals[1], 3)}\\t{round(vals[2], 3)}\\t{round(vals[3], 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "    # Use train/eval to see how well the bm25 works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Best result from each query\n",
    "# results = {}\n",
    "# t = 0\n",
    "# # Some tokenized queries are the same\n",
    "# for query, tokenized in zip(queries, tokenized_queries):\n",
    "#     t += 1\n",
    "#     scores = bm25.get_scores(tokenized)\n",
    "#     d = {}\n",
    "#     for id, i in enumerate(scores):\n",
    "#         if i:\n",
    "#             d[str(id)] = 1\n",
    "#     results[str(query_lookup[query])] = d\n",
    "# print(len(results))\n",
    "# print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultater\n",
    "- Se M5-retrieval_evaluation for å se hvordan man skal sammenlikne rank og ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qrels_dict(qrels_df : pd.DataFrame) -> dict[str, dict[str, int]]:\n",
    "    qrels_dict = {}\n",
    "    for idx, row in qrels_df.iterrows():\n",
    "        qid = str(row[0])\n",
    "        pid = str(row[2])\n",
    "        relevance = row[3]\n",
    "\n",
    "        if qid not in qrels_dict:\n",
    "            qrels_dict[qid] = {}\n",
    "        qrels_dict[qid][pid] = relevance\n",
    "\n",
    "    return qrels_dict\n",
    "\n",
    "qrels_dict = create_qrels_dict(qrels_dev_df)\n",
    "# qrels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranx import Qrels, Run\n",
    "\n",
    "\n",
    "qrels = Qrels(q)\n",
    "\n",
    "## Create a run\n",
    "run = Run(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'map@5': 0.0, 'mrr': 0.0}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ranx import evaluate\n",
    "\n",
    "# Compute score for a single metric\n",
    "evaluate(qrels, run, \"ndcg@5\")\n",
    "\n",
    "\n",
    "# Compute scores for multiple metrics at once\n",
    "evaluate(qrels, run, [\"map@5\", \"mrr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranx import compare\n",
    "\n",
    "# Compare different runs and perform Two-sided Paired Student's t-Test\n",
    "report = compare(\n",
    "    qrels=qrels,\n",
    "    runs=[run_1, run_2, run_3, run_4, run_5],\n",
    "    metrics=[\"map@100\", \"mrr@100\", \"ndcg@10\"],\n",
    "    max_p=0.01  # P-value threshold\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afea23f61d6a821af1961b60809105e43be664e14901e921a7e345d902bc2549"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
